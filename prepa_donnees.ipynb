{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5faab8c3-edde-4a0e-a93b-5c8ce0567e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "grille = gpd.read_file(\"./data/grille1km_metropole.gpkg\")\n",
    "communes = gpd.read_file(\"./data/commune_francemetro_2023.gpkg\")\n",
    "# 1. Intersection complète (Overlay)\n",
    "# Cela découpe les carreaux aux frontières exactes des communes\n",
    "pieces = gpd.overlay(grille, communes, how='intersection')\n",
    "\n",
    "# 2. Calculer la surface de chaque morceau\n",
    "pieces['area_piece'] = pieces.geometry.area\n",
    "\n",
    "# 3. Trier pour mettre les plus gros morceaux en premier\n",
    "pieces = pieces.sort_values('area_piece', ascending=False)\n",
    "\n",
    "# 4. Pour chaque ID_Carreau, ne garder que le premier (le plus gros)\n",
    "\n",
    "attribution_finale = pieces.drop_duplicates(subset=['id_carr_1km'], keep='first')\n",
    "\n",
    "# 5. Nettoyage : on ne garde que l'ID carreau et l'ID commune \n",
    "mapping_carreau_commune = attribution_finale[['id_carr_1km', 'code']]\n",
    "mapping_carreau_commune.to_parquet(\"./data/mapping_carreau_commune.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2128661c-75d2-4d09-9a60-760cb362dc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['code', 'libelle', 'cheflieu', 'tncc', 'dct', 'dar', 'z20', 'duu20',\n",
       "       'aav20', 'typo_aav20', 'bv22', 'epc', 'ctcd', 'dep', 'reg', 'surf',\n",
       "       'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166f7edd-d6bd-4c9f-ac80-5d35dc4ecb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.read_parquet(\"./data/grid_1km.parquet\")\n",
    "grid = grid[['GRD_ID', 'TOT_P_2018', 'TOT_P_2006',  'TOT_P_2011',\n",
    "       'TOT_P_2021']]\n",
    "grille = gpd.read_file(\"./data/grille1km_metropole.gpkg\")\n",
    "\n",
    "grille = grille.merge(grid, left_on='id_carr_1km',right_on='GRD_ID', how='left').drop(['idINSPIRE', 'GRD_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74bcf16f-2b5f-440f-afdb-d7bb89da388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filo19 = pd.read_csv(\"./data/filo_2019_carreaux_1km_met.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e2fe95-05ed-4ec8-bbb5-5947d3490379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['idcar_1km', 'i_est_1km', 'lcog_geo', 'ind', 'men', 'men_pauv',\n",
       "       'men_1ind', 'men_5ind', 'men_prop', 'men_fmp', 'ind_snv', 'men_surf',\n",
       "       'men_coll', 'men_mais', 'log_av45', 'log_45_70', 'log_70_90',\n",
       "       'log_ap90', 'log_inc', 'log_soc', 'ind_0_3', 'ind_4_5', 'ind_6_10',\n",
       "       'ind_11_17', 'ind_18_24', 'ind_25_39', 'ind_40_54', 'ind_55_64',\n",
       "       'ind_65_79', 'ind_80p', 'ind_inc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filo19.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5538d141-00d5-46fc-a895-315372504424",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing geo metadata in Parquet/Feather file.\n            Use pandas.read_parquet/read_feather() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stats = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/stats_1km.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/geopandas/io/arrow.py:762\u001b[39m, in \u001b[36m_read_parquet\u001b[39m\u001b[34m(path, columns, storage_options, bbox, to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    759\u001b[39m path = _expand_user(path)\n\u001b[32m    760\u001b[39m schema, metadata = _read_parquet_schema_and_metadata(path, filesystem)\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m geo_metadata = \u001b[43m_validate_and_decode_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(geo_metadata[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m]) == \u001b[32m0\u001b[39m:\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    765\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"No geometry columns are included in the columns read from\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[33;03m        the Parquet/Feather file.  To read this file without geometry columns,\u001b[39;00m\n\u001b[32m    767\u001b[39m \u001b[33;03m        use pandas.read_parquet/read_feather() instead.\"\"\"\u001b[39;00m\n\u001b[32m    768\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/geopandas/io/arrow.py:619\u001b[39m, in \u001b[36m_validate_and_decode_metadata\u001b[39m\u001b[34m(metadata)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_and_decode_metadata\u001b[39m(metadata):\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgeo\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m metadata:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    620\u001b[39m \u001b[38;5;250m            \u001b[39m\u001b[33;03m\"\"\"Missing geo metadata in Parquet/Feather file.\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[33;03m            Use pandas.read_parquet/read_feather() instead.\"\"\"\u001b[39;00m\n\u001b[32m    622\u001b[39m         )\n\u001b[32m    624\u001b[39m     \u001b[38;5;66;03m# check for malformed metadata\u001b[39;00m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Missing geo metadata in Parquet/Feather file.\n            Use pandas.read_parquet/read_feather() instead."
     ]
    }
   ],
   "source": [
    "stats = pd.read_parquet(\"./data/stats_1km.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f69871c-bdd2-44e2-ab2b-539465339da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = stats.merge(grille, how='left', on=\"id_carr_1km\")\n",
    "stats = stats.merge(filo19, how='left', left_on = 'id_carr_1km', right_on='idcar_1km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed51e5e9-5a01-44d5-bf52-56192300b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tissu Urbain (La \"Masse\" bâtie)\n",
    "stats['struct_bati'] = stats['part_classe_1'] + stats['part_classe_2']\n",
    "\n",
    "# 2. Zones d'Activité (Emploi / ZI / Commercial / Routes)\n",
    "stats['struct_eco'] = stats['part_classe_3'] + stats['part_classe_4']\n",
    "\n",
    "# 3. Nature (Forêts et milieux semi-naturels)\n",
    "# Classes 11 à 19 (Forêts, Landes, Fourrés...)\n",
    "# Astuce : on utilise une liste pour être sûr de tout prendre\n",
    "cols_nature = [f'part_classe_{i}' for i in range(11, 20)] \n",
    "stats['struct_nature'] = stats[cols_nature].sum(axis=1)\n",
    "\n",
    "# 4. Agriculture (Champs - Réserve foncière)\n",
    "# Classes 20 à 22 (Cultures, Prairies, Vergers)\n",
    "cols_agri = [f'part_classe_{i}' for i in range(20, 23)]\n",
    "stats['struct_agri'] = stats[cols_agri].sum(axis=1)\n",
    "\n",
    "# 5. Eau & Glaciers (Hydrosphère)\n",
    "# On peut grouper Eau (23) et Glaciers (24) ou les séparer.\n",
    "# Pour la morpho pure, les séparer est intéressant (le glacier est une contrainte \"montagne\", l'eau une \"aménité\")\n",
    "stats['struct_eau'] = stats['part_classe_23']\n",
    "stats['struct_glacier'] = stats['part_classe_24'] # Spécifique Alpes\n",
    "\n",
    "# Verification : La somme devrait faire (1 - part_classe_0)\n",
    "# Cela vous permet de vérifier l'intégrité des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c320e9-5454-4038-8466-924f59a77270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Densité Humaine (Indicateur de charge)\n",
    "# On divise par la surface du carreau (qui est de 1km², donc c'est déjà une densité)\n",
    "stats['densite_pop'] = stats['TOT_P_2021'] # Ou 'ind' de Filosofi\n",
    "\n",
    "# 2. Richesse Relative\n",
    "# Attention : ind_snv est une SOMME. Il faut diviser par le nombre d'individus.\n",
    "# Remplacer les 0 par NaN temporairement pour éviter l'erreur, puis fillna\n",
    "stats['niveau_vie_moyen'] = (stats['ind_snv'] / stats['ind'].replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# 3. Précarité Structurelle\n",
    "stats['taux_pauvrete'] = (stats['men_pauv'] / stats['men'].replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# 4. Morphologie Sociale (Propriétaire vs Locataire)\n",
    "stats['part_proprio'] = (stats['men_prop'] / stats['men'].replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# 5. Type d'Habitat (Morphologie Bâtie vue par l'INSEE)\n",
    "stats['part_maison'] = (stats['men_mais'] / stats['men'].replace(0, np.nan)).fillna(0)\n",
    "stats['part_hlm'] = (stats['log_soc'] / (stats['men_coll'] + stats['men_mais']).replace(0, np.nan)).fillna(0)\n",
    "\n",
    "# 6. Dynamique Démographique (Votre touche temporelle !)\n",
    "# Croissance sur 10 ans (2011-2021)\n",
    "stats['croissance_pop'] = (stats['TOT_P_2021'] - stats['TOT_P_2011']) / stats['TOT_P_2011'].replace(0, np.nan)\n",
    "stats['croissance_pop'] = stats['croissance_pop'].fillna(0)\n",
    "stats['is_imputed'] = (stats['i_est_1km'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9119a9bf-0d16-40d4-8a1b-3c61f1b12a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = stats[['id_carr_1km','z_mean', 'z_std', 'slope_mean','struct_bati',\n",
    "       'struct_eco', 'struct_nature', 'struct_agri', 'struct_eau',\n",
    "       'struct_glacier','densite_pop', 'niveau_vie_moyen', 'taux_pauvrete',\n",
    "       'part_proprio', 'part_maison', 'part_hlm', 'croissance_pop','is_imputed','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "488dba0e-4176-4bea-898e-412b92b7c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0d5ebd9-c1c4-4999-8ac9-4fc011332670",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_carreau_commune = pd.read_parquet(\"./data/mapping_carreau_commune.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e26154b-994a-4199-ba78-41b0eaa70bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.merge(mapping_carreau_commune, how='left', on='id_carr_1km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d121751-c37d-43d0-b2bb-9feab022935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = gpd.GeoDataFrame(df_final, geometry=df_final['geometry'], crs=\"EPSG:2154\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d4151-4d58-42ca-8103-a8f3adb9083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Isoler les orphelins et les bien classés\n",
    "# On suppose que votre DF s'appelle 'df_final' et est un GeoDataFrame\n",
    "orphelins = df_final[df_final['code'].isna()].copy()\n",
    "bien_classes = df_final[~df_final['code'].isna()].copy()\n",
    "\n",
    "print(f\"Nombre d'orphelins à sauver : {len(orphelins)}\")\n",
    "\n",
    "# 2. Charger le fond de carte des communes (Contours officiels IGN ou Admin Express)\n",
    "# Il faut absolument les polygones des communes pour calculer la distance\n",
    "communes = communes[['code', 'geometry']] \n",
    "\n",
    "# 3. Spatial Join \"Nearest\" (Le Sauvetage)\n",
    "# On cherche pour chaque orphelin la commune la plus proche\n",
    "# Attention : assurez-vous d'être dans le même CRS (ex: EPSG:2154 pour la France)\n",
    "sauvetage = gpd.sjoin_nearest(\n",
    "    orphelins.drop(columns=['code']), # On enlève la colonne vide pour la remplacer\n",
    "    communes,\n",
    "    how='left',\n",
    "    distance_col='dist_to_commune' # Utile pour vérifier qu'on ne va pas chercher trop loin\n",
    ")\n",
    "\n",
    "# 4. Nettoyage post-sauvetage\n",
    "# Si la distance est trop grande (ex: > 2km), c'est que c'est un vrai bug ou un carreau\n",
    "# perdu au milieu de la Suisse. On peut décider de les jeter.\n",
    "seuil_dist = 2000 # mètres\n",
    "sauvetage_valide = sauvetage[sauvetage['dist_to_commune'] < seuil_dist].copy()\n",
    "\n",
    "# Renommer la colonne récupérée (INSEE_COM) en lcog_geo pour matcher l'autre table\n",
    "# sauvetage_valide = sauvetage_valide.rename(columns={'INSEE_COM': 'lcog_geo'})\n",
    "\n",
    "# 5. Re-fusionner tout le monde\n",
    "cols_to_keep = bien_classes.columns # On garde la même structure\n",
    "df_complet = pd.concat([bien_classes, sauvetage_valide[cols_to_keep]])\n",
    "\n",
    "print(f\"Total final : {len(df_complet)} carreaux.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d816b8b8-0d47-477e-9034-f396d26afdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complet.to_parquet(\"./data_GNN/statistiques_carreaux.parquet.gz\", compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
